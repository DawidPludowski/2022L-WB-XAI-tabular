---
title: "Case Studies 2022L Lab 3"
author: "Mustafa Cavus"
date: "Mar 24, 2022"
output: html_document
---

## Installing the necessary packages

* {DALEX} for creating explainer and calling the dataset
* {tidyverse} for using glimpse()
* {caret} for splitting the dataset to train and test set
* {ranger} for traning random forest model

```{r message=FALSE, warning=FALSE, include=FALSE}
#install.packages("DALEX")
#install.packages("tidyverse")
#install.packages("caret")
#install.packages("ranger")

library(DALEX)
library(tidyverse)
library(caret)
library(ranger)
```

## Training the model

We will use the apartments dataset from {DALEX}

```{r}
data("apartments")
head(apartments)
```

Splitting the dataset

We can use createDataPartition() from {caret} package

```{r}
set.seed(123)
# create the sample row index for train and test set
index <- createDataPartition(apartments$m2.price, p = 0.8, list = FALSE)

# using index split to data to train and test set
train <- apartments[index,]
test  <- apartments[-index,]
```


# Training a random forest model

ranger() is the fast implementation of the random forest model based on C++. By using it, you can train your model quicker than the usual one.

You need to put target and train set as arguments. Then calculate the predicted values of the model on test set and compare the values of evaluation metrics.

```{r}
set.seed(123)
ranger_model <- ranger(m2.price ~., data = train)
```


# Create an explainer

In DALEX, we need to create an explainer of the model first. 

```{r}
explainer_rf <- DALEX::explain(ranger_model, 
                               data = test[,-1],  
                               y = test$m2.price,
                               label = "random forest")
```


# LIME method

LIME and its variants are implemented in various R and Python packages. In this notebook, we will consider two of them: {lime} and {localModel} in R.

Because the functions used to calculate the LIME in these packages are different, we will use predict_surrogate() from {DALEXtra} provides a uniform interface to these functions.

## The {lime} package

Let's first install and call the necessary packages.

```{r message=FALSE, warning=FALSE, include=FALSE}
#install.packages("DALEXtra")
#install.packages("lime")

library(DALEXtra)
library(lime)
```

predict_surrogate() function has two obligatory arguments: explainer and new_observation. Depend on choosing of the type argument, there are extra arguments we must provide. In "lime", the extra arguments:

* n_features: the maximum number of explanatory variables to be selected by K-LASSO method.
* n_permutations: the number of artificial data points for local-model.

```{r}
model_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer
predict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer

lime_pr <- predict_surrogate(explainer = explainer_rf, 
                             new_observation = as.data.frame(apartments[53,-1]), 
                             n_features = 3, 
                             n_permutations = 1000,
                             type = "lime")
```

Let's plot it!

```{r}
lime_pr
plot(lime_pr)
```

Let's read the output of lime_pr:

* The output includes column 'case' that provides indices of observations for which the explanations are calculated. In our case there is only one index equal to 1, because we asked for an explanation for only one observation, apartments[53,]. 

* 'feature' column indicates which explanatory variables were given non-zero coefficients in the K-LASSO method. 

* 'feature_value' column provides information about the values of the original explanatory variables for the observations for which the explanations are calculated. 

* 'feature_desc' column indicates how the original explanatory variable was transformed. Note that the applied implementation of the LIME method dichotomizes continuous variables by using quartiles. Hence, for instance, surface and construction.year for the selected observation was transformed into categorial and binary variable, respectively.

* 'feature_weight' provides the estimated coefficients for the variables selected by the K-LASSO method for the explanation. 

* 'model_intercept' column provides of the value of the intercept. Thus, the linear combination of the transformed explanatory variables used in the glass-box model approximating the random forest model around the instance of interest.

* "Explanation fit" refers to the R2 of the linear model that is fitted locally to explain the variance in the neighbourhood of the examined case.


## The {localModel} package

Let's first install and call the necessary packages.

```{r message=FALSE, warning=FALSE, include=FALSE}
#install.packages("localModel")
library(localModel)
```

In "localModel", the extra arguments:

* size: the number of artificial data points to be sampled for the local-model approximation.
* seed: the seed for the random-number generation.

```{r}
localModel_pr <- predict_surrogate(explainer = explainer_rf, 
                                   new_observation = as.data.frame(apartments[53,-1]), 
                                   size = 1000, 
                                   seed = 123,
                                   type = "localModel")
```


Let's plot it!

```{r}
localModel_pr
plot(localModel_pr)
```

To read the output:

* 'estimated' column contains the estimated coefficients of the LASSO regression model, which is used to approximate the predictions from the random forest model. 

* 'variable' provides the information about the corresponding variables, which are transformations of original_variable.


Also the profile for a variable of the selected observation can be obtained by 'plot_interpretable_feature()'.

```{r}
plot_interpretable_feature(localModel_pr, "construction.year")
```

The profile indicates that the largest increase in the predicted value of m2.price is observed when the value of construction.year about 1995. Hence, in the output of the predict_surrogate() function, we see a binary variable construction.year > 1995.


# Application

Let's try explain a specific apartment on your own with the LIME method.


